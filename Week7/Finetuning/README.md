### Comparing Word2Vec with Transformer-based Embeddings for IP Flows  
* This was a quick experiment to see whether Word2Vec, trained from scratch on IP flow tokens, could perform comparably to transformer models like distilbert-base-nli-stsb-mean-tokens (this was one of the models that trained th efastest)-  without any additional fine-tuning.  
* The motivation here was simple: if a shallow embedding method like Word2Vec performs similarly in clustering, we can save a ton of computation and memory (since it doesnt have as many dimensions as the bert based models).  
* To reduce variance in frame length of the dataset, it was bucketed into 3 categories: small, medium, and large- this simplifies the token space for Word2Vec.  

#### Constructing the "Sentence" per Flow  
* For both models (Word2Vec and BERT), each row of the dataset was converted into a structured sentence like:  
  "src:172.16.0.1 dst:10.0.0.5 sport:443 dport:80 proto:TCP len:medium"
* These tokenized flows were then either:  
    - Fed into Word2Vec (as a list of tokens)  
    - Fed into a transformer model (as a single string joined by spaces)

#### Word2Vec Training  
* Word2Vec was trained from scratch using skip-gram, vector_size=10, and a window size of 2.  
* Training was done for 50 epochs over all flow sentences.  
* The idea was to build a basic semantic space of flow elements like IP addresses, ports, and length buckets.

#### Synthetic Flow Generation  
* Instead of random IPs, four flow families were defined to simulate real-world network behavior:  
    1. video: high-volume, TCP, port 443, huge packet length  
    2. web: medium-length, HTTP, port 80  
    3. dns: small packets, UDP, port 53  
    4. ssh: very small packets, TCP, port 22  
* Each flow family had fixed behavior patterns to test whether clustering based on embeddings could recover the underlying group that it belonged to 

#### Embedding and Clustering  
* Word2Vec embeddings were computed by taking the mean of token vectors per flow.  
* Transformer embeddings were generated by encoding the entire sentence using distilbert and normalizing the output.  
* For both sets of embeddings:  
    - Clustering was done using KMeans (k=4)
    - Evaluation was done via Purity Score and Silhouette Score

#### Results  
Silhouette (Word2Vec): 0.3368
Silhouette (Trans): 0.3477

Purity (Word2Vec): 0.6800
Purity (Transformer): 1.0000

* The PCA visualizations of both embedding spaces showed clearer cluster separation in the transformer space.  
* Word2Vec still managed to form rough groups, but the clusters were looser, and some families (DNS vs SSH) got muddled 
* Silhouette score for Word2Vec was lower- indicating that even when clusters formed, they were less "tight" than BERT's.


#### Conclusion
* Word2Vec is good for lightweight representation and gave kinda reassonable cluters  
* But transformer-based embeddings were significantly better at separating subtle flow patterns.  
* Since transformer models generalize better and handle ordering/context more naturally, theyâ€™re likely the better choice for flow analysis- especially if fine-grained classification is the end goal.  
* That said, Word2Vec could still be a valid option for memory-constrained environments or edge devices where inference cost matters more than precision- **so should this be looked into more?**