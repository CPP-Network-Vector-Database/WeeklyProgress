# -*- coding: utf-8 -*-
"""ip2vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/CPP-Network-Vector-Database/WeeklyProgress/blob/main/Week3-4/FAISS/ip2vec.ipynb
"""

pip install torch transformers faiss-cpu pandas

pip install sentence-transformers

import pandas as pd
import faiss
import numpy as np
import gensim
from gensim.models import Word2Vec
import time
import psutil
import os

csv_path = "/kaggle/input/pcap-2019-dira-125910/dirA.125910-packets.csv"
df = pd.read_csv(csv_path, header=None, names=["timestamp", "src_ip", "dst_ip", "protocol", "size"])

df["ip_sequence"] = df["src_ip"] + " " + df["dst_ip"]

# Tokenize sequences
ip_sequences = [flow.split() for flow in df["ip_sequence"]]

# Train IP2Vec using Word2Vec
ip2vec_model = Word2Vec(sentences=ip_sequences, vector_size=128, window=5, min_count=1, workers=4)
ip2vec_model.save("ip2vec.model")

unique_ips = list(set(df["src_ip"].tolist() + df["dst_ip"].tolist()))
ip_embeddings = {ip: ip2vec_model.wv[ip] for ip in unique_ips if ip in ip2vec_model.wv}

embeddings = np.array(list(ip_embeddings.values()))
embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

dimension = embeddings.shape[1]  # 128-dimensional embeddings
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"FAISS index contains {index.ntotal} unique IP embeddings.")

faiss.write_index(index, "ip_embeddings.index")

ip_metadata = pd.DataFrame({"ip": list(ip_embeddings.keys())})
ip_metadata.to_csv("ip_metadata.csv", index=False)

print("IP2Vec model, FAISS index, and metadata saved.")

ip2vec_model = Word2Vec.load("ip2vec.model")

index = faiss.read_index("ip_embeddings.index")
ip_metadata = pd.read_csv("ip_metadata.csv")

# IP metadata to dictionary for lookup
ip_to_index = {ip: i for i, ip in enumerate(ip_metadata["ip"])}

"""## Performance logging"""

import time
import faiss
import numpy as np
import pandas as pd
import psutil
from gensim.models import Word2Vec
import os

def measure(pid, func, *args, **kwargs):
    process = psutil.Process(pid)

    start_cpu = process.cpu_percent(interval=None)
    start_mem = process.memory_info().rss / (1024 ** 2)  # Convert to MB
    start_time = time.time()

    result = func(*args, **kwargs)

    end_cpu = process.cpu_percent(interval=None)
    end_mem = process.memory_info().rss / (1024 ** 2)  # Convert to MB
    end_time = time.time()

    cpu_usage = end_cpu - start_cpu
    mem_usage = end_mem - start_mem
    execution_time = end_time - start_time

    print(f"Function: {func.__name__} | Time: {execution_time:.4f}s | CPU: {cpu_usage:.2f}% | Mem: {mem_usage:.2f}MB")
    return result

"""### 1. Queryng a new packet
Steps:
1. Convert the query packet into text format.
2. Generate its BERT embedding.
3. Normalize the embedding (since FAISS works best with normalized vectors).
4. Search the FAISS index for the k-nearest neighbors.
5. Return the top-k results with their distances (lower = more similar).
"""

def query_faiss(index, query_embedding, k=5):
    distances, indices = index.search(query_embedding, k)
    return indices

query_ip = "26.120.99.176" #this is the first entry in the dataset
# query_ip = "192.168.1.1" # this is not in the dataset

if query_ip in ip2vec_model.wv:
    query_embedding = ip2vec_model.wv[query_ip].reshape(1, -1)
    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)

    pid = os.getpid()
    query_result = measure(pid, query_faiss, index, query_embedding, 5)

    similar_ips = [ip_metadata.iloc[i]["ip"] for i in query_result[0]]
    print(f"Top 5 similar IPs to {query_ip}: {similar_ips}")
else:
    print(f"IP {query_ip} not found in model.")

"""### 2. Large Insertions
Steps:
1. Convert 50+ new packets into embeddings.
2. Normalize them.
3. Add to the FAISS index.
"""

def insert_data(index, new_ip_embeddings, new_ips):
    print(f"Inserting {len(new_ips)} IPs into FAISS index")
    index.add(new_ip_embeddings)
    return new_ips

# all_ips = list(ip2vec_model.wv.key_to_index.keys())
# print(f"Total IPs in model: {len(all_ips)}")
# print("Sample IPs:", all_ips[:10])

new_ips = [f"205.190.20.{i}" for i in np.random.choice(range(171), 50, replace=False)]

embedding_dim = ip2vec_model.vector_size  # Get the embedding size
new_embeddings = np.random.rand(len(new_ips), embedding_dim)  # Create random vectors

# Normalize the embeddings
new_embeddings = new_embeddings / np.linalg.norm(new_embeddings, axis=1, keepdims=True)

# print(f"New Random IP: {new_ips}")
# print(f"New embddings shape: {new_embeddings.shape}")

pid = os.getpid()
inserted_ips = measure(pid, insert_data, index, new_embeddings, new_ips)

new_ip_metadata = pd.DataFrame({"ip": new_ips})
ip_metadata = pd.concat([ip_metadata, new_ip_metadata], ignore_index=True)
ip_metadata.to_csv("ip_metadata.csv", index=False)

"""### 3. Large Deletions
**Problem!!!**
**FAISS does not support direct deletion of individual embeddings.**

Workaround:
1. Remove entries from the metadata CSV.
2. Rebuild the FAISS index without the deleted embeddings.
"""

def delete_from_faiss(index, delete_indices, embeddings):
    #delete by rebuilding the index with the remaining embeddings.
    if not isinstance(delete_indices, np.ndarray):
        delete_indices = np.array(delete_indices)

    # Ensure delete_indices are valid
    delete_indices = delete_indices[delete_indices < embeddings.shape[0]]

    # Create a mask for filtering
    mask = np.ones(embeddings.shape[0], dtype=bool)
    mask[delete_indices] = False

    # Select embeddings that are not deleted
    new_embeddings = embeddings[mask]

    # Rebuild the FAISS index w remaining embeddings
    new_index = faiss.IndexFlatL2(new_embeddings.shape[1])
    new_index.add(new_embeddings)

    return new_index, new_embeddings

delete_indices = np.random.choice(embeddings.shape[0], 50, replace=False)

pid = os.getpid()
index, embeddings = measure(pid, delete_from_faiss, index, delete_indices, embeddings)
print(f"Deleted {len(delete_indices)} embeddings from FAISS index.")

"""### 4. Large Updates
Similar to delete + insert.

Just remove old embeddings from FAISS, recompute new ones, and reinsert.
"""

all_ips = list(ip2vec_model.wv.key_to_index.keys())
valid_ips = all_ips[:50]
# print(f"Valid IPs: {valid_ips}")

def increment_last_digit(ip):
    parts = ip.split(".")
    parts[-1] = str((int(parts[-1]) + 1) % 256)
    return ".".join(parts)

modified_ips = [increment_last_digit(ip) for ip in valid_ips]
print(f"Modified IPs: {modified_ips}")

#new embeddings
new_embeddings = np.array([ip2vec_model.wv[ip] for ip in modified_ips if ip in ip2vec_model.wv])

if new_embeddings.shape[0] > 0:
    new_embeddings = new_embeddings / np.linalg.norm(new_embeddings, axis=1, keepdims=True)

    pid = os.getpid()
    update_indices = np.arange(len(valid_ips))
    index, embeddings = measure(pid, update_faiss, index, update_indices, modified_ips, embeddings)

    print(f"Updated embeddings in FAISS index.")

